{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0913dbf",
   "metadata": {},
   "source": [
    "## Machine Learning Project 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8813aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scikit-learn pereptron and adaline implementations\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from itertools import combinations\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62eb727e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('/Users/jaxen1/Shared/C/Machine Learning/project2/project_adult.csv')\n",
    "validation = pd.read_csv('/Users/jaxen1/Shared/C/Machine Learning/project2/project_validation_inputs.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310d16ea",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee310dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess data\n",
    "def preprocess_data(df):\n",
    "    # Take out educational since is ordinal, ordinal var is already in dataset\n",
    "    df = df.drop(columns = ['Unnamed: 0', 'education'])\n",
    "    # Handle missing values\n",
    "    df = df.dropna()\n",
    "    # Replace all values unknown with most common ('Private')\n",
    "    df['workclass'] = df['workclass'].apply(lambda x: df['workclass'].value_counts().index[0] if x == '?' else x)\n",
    "\n",
    "    # TRY EXCEPT SO VALIDATION SET CAN PASS\n",
    "    # Binarize the target variable\n",
    "    try:\n",
    "        df['income'] = df['income'].apply(lambda x: 1 if x == '>50K' else 0)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # keep track of rows indexes to connect X and y\n",
    "    df = df.reset_index()\n",
    "\n",
    "    # Deal with Categorical Values\n",
    "\n",
    "    # initialize encoder\n",
    "    encoder = OneHotEncoder(sparse_output=False)\n",
    "    # pull categorical cols\n",
    "    categorical_cols = df.select_dtypes(include='object').columns\n",
    "    # create encoded array w one hot columns\n",
    "    encoded_array = encoder.fit_transform(df[categorical_cols])\n",
    "    # Get new column names\n",
    "    encoded_cols = encoder.get_feature_names_out(categorical_cols)\n",
    "    # Creae new df of encoded columns\n",
    "    encoded_df = pd.DataFrame(encoded_array, columns=encoded_cols, index=df.index)\n",
    "    # Drop the original cat cols, add the one hot encoded one\n",
    "    df = df.drop(categorical_cols, axis=1)\n",
    "    df = pd.concat([df, encoded_df], axis=1)\n",
    "    df = df.drop('index', axis = 1)\n",
    "\n",
    "    # Ensure all columns are numeric after one-hot encoding\n",
    "    for col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "    # TRY EXCEPTS SO THE VALIDATION SET CAN PASS WITH NO Y VARIABLE\n",
    "    # Separate features and target\n",
    "    try:\n",
    "        X = df.drop(columns=['income'], axis=1)\n",
    "    except:\n",
    "        X = df\n",
    "    try:\n",
    "        y = df['income']\n",
    "    except:\n",
    "        y = None\n",
    "\n",
    "    # Standardize numerical features\n",
    "    numeric_cols = ['age','fnlwgt','capital-gain','capital-loss', 'hours-per-week']\n",
    "    scaler = StandardScaler()\n",
    "    # Ensure numeric columns exist before scaling\n",
    "    numeric_cols_exist = [col for col in numeric_cols if col in X.columns]\n",
    "    if numeric_cols_exist:\n",
    "        X[numeric_cols_exist] = scaler.fit_transform(X[numeric_cols_exist])\n",
    "\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d16943c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run function on both datasets\n",
    "X, y = preprocess_data(data)\n",
    "X_validation, _ = preprocess_data(validation)\n",
    "# Ensure validation set has same columns as training set and in right order\n",
    "missing_cols = [\n",
    "    'native-country_Holand-Netherlands',\n",
    "    'native-country_Outlying-US(Guam-USVI-etc)',\n",
    "    'workclass_Never-worked'\n",
    "]\n",
    "for col in missing_cols:\n",
    "    X_validation[col] = 0\n",
    "X_validation = X_validation.reindex(columns=X.columns, fill_value=0)\n",
    "# Split the data 80-20\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=17)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9ba13e",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100cb567",
   "metadata": {},
   "source": [
    "This will need to be edited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2d0d65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMLP:\n",
    "\n",
    "    def __init__(self, num_features, num_hidden, num_classes, weight_h: Optional[np.ndarray] = None, bias_h: Optional[np.ndarray] = None, weight_out: Optional[np.ndarray] = None, bias_out: Optional[np.ndarray] = None, random_seed=17):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        Simple MLP Classifier with 1 hidden layer\n",
    "\n",
    "        Parameters\n",
    "        ------------\n",
    "        num_features : int\n",
    "          number of features in data\n",
    "        num_hidden : int\n",
    "          number of nodes in the hidden layer\n",
    "        num_classes : int\n",
    "          number of classes in response variable\n",
    "        weight_h : array\n",
    "          array of weights for hidden layer weights\n",
    "        bias_h : array\n",
    "          array of weights for hidden layer bias\n",
    "        weight_out : array\n",
    "          array of weights for output layer weights\n",
    "        bias_out : array\n",
    "          array of weights for output layer bias\n",
    "        random_seed : int\n",
    "          seed to use for random number generation for weights    \n",
    "        \"\"\"\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # hidden\n",
    "        if weight_h is None:\n",
    "            print(\"creating random weights for weight_h\")\n",
    "            rng = np.random.RandomState(random_seed)\n",
    "            \n",
    "            self.weight_h = rng.normal(\n",
    "                loc=0.0, scale=0.1, size=(num_hidden, num_features))\n",
    "            self.bias_h = np.zeros(num_hidden)\n",
    "        else: \n",
    "            self.weight_h = weight_h\n",
    "\n",
    "        if bias_h is None:\n",
    "            print(\"creating random weights for bias_h\")\n",
    "            self.bias_h = np.zeros(num_hidden)\n",
    "        else: \n",
    "            self.bias_h = bias_h\n",
    "        \n",
    "        # output\n",
    "        if weight_out is None:\n",
    "            print(\"creating random weights for weight_out\")\n",
    "            self.weight_out = rng.normal(\n",
    "                loc=0.0, scale=0.1, size=(num_classes, num_hidden))\n",
    "        else: \n",
    "            self.weight_out = weight_out\n",
    "\n",
    "        if bias_out is None:\n",
    "            print(\"creating random weights for bias_out\")\n",
    "            self.bias_out = np.zeros(num_classes)\n",
    "        else: \n",
    "            self.bias_out = bias_out\n",
    "    \n",
    "    \"\"\"\n",
    "    USE DIFFERENT ACTIVATION FUNCTIONS\n",
    "\n",
    "    COMPARE:\n",
    "    IDENTITY\n",
    "    SIGMOID\n",
    "    SOFTMAX\n",
    "    RELU\n",
    "    TANH\n",
    "    \"\"\"\n",
    "    \n",
    "    def activation(self, X):\n",
    "        \"\"\"Compute linear activation\"\"\"\n",
    "        return X\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Hidden layer\n",
    "        # input dim: [n_examples, n_features] dot [n_hidden, n_features].T\n",
    "        # output dim: [n_examples, n_hidden]\n",
    "        z_h = np.dot(x, self.weight_h.T) + self.bias_h\n",
    "        #print(f\"z_h = {x} dot {self.weight_h.T} + {self.bias_h}\")\n",
    "        a_h = self.activation(z_h)\n",
    "        #print(f\"a_h: {a_h}\")\n",
    "\n",
    "        # Output layer\n",
    "        # input dim: [n_examples, n_hidden] dot [n_classes, n_hidden].T\n",
    "        # output dim: [n_examples, n_classes]\n",
    "        z_out = np.dot(a_h, self.weight_out.T) + self.bias_out\n",
    "        #print(f\"z_out = {a_h} dot {self.weight_out.T} + {self.bias_out}\")\n",
    "        a_out = self.activation(z_out)\n",
    "        #print(f\"a_out: {a_out}\")\n",
    "        return a_h, a_out\n",
    "\n",
    "    def backward(self, x, a_h, a_out, y_onehot):  \n",
    "\n",
    "        # assuming onehot for y\n",
    "  \n",
    "        # Part 1: dLoss/dOutWeights\n",
    "        ## = dLoss/dOutAct * dOutAct/dOutNet * dOutNet/dOutWeight\n",
    "        ## where DeltaOut = dLoss/dOutAct * dOutAct/dOutNet\n",
    "        ## for convenient re-use\n",
    "        \n",
    "        # input/output dim: [n_examples, n_classes]\n",
    "        d_loss__d_a_out = (a_out - y_onehot)\n",
    "\n",
    "        # input/output dim: [n_examples, n_classes]\n",
    "        d_a_out__d_z_out = 1\n",
    "\n",
    "        # [n_examples, n_hidden]\n",
    "        d_z_out__dw_out = a_h\n",
    "        \n",
    "        # gradient for output weights\n",
    "        # want output dim: [n_classes, n_hidden]\n",
    "        # current have [n_examples, n_classes] * [n_examples, n_classes] * [n_examples, n_hidden], which isn't possible. \n",
    "        # combine matrix with same [n_examples, n_classes]\n",
    "        # output dim: [n_examples, n_classes]\n",
    "        delta_out = d_loss__d_a_out * d_a_out__d_z_out # \"delta (rule) placeholder\"\n",
    "\n",
    "        # input dim: [n_classes, n_examples] dot [n_examples, n_hidden]\n",
    "        # output dim: [n_classes, n_hidden]\n",
    "        d_loss__dw_out = np.dot(delta_out.T, d_z_out__dw_out)\n",
    "\n",
    "        # gradient for output bias\n",
    "        d_loss__db_out = np.sum(delta_out, axis=0)\n",
    "        \n",
    "\n",
    "        #################################        \n",
    "        # Part 2: dLoss/dHiddenWeights\n",
    "        ## = DeltaOut * dOutNet/dHiddenAct * dHiddenAct/dHiddenNet * dHiddenNet/dWeight\n",
    "        \n",
    "        # [n_classes, n_hidden]\n",
    "        d_z_out__a_h = self.weight_out\n",
    "        \n",
    "        # output dim: [n_examples, n_hidden]\n",
    "        d_loss__a_h = np.dot(delta_out, d_z_out__a_h)\n",
    "        \n",
    "        # [n_examples, n_hidden]\n",
    "        d_a_h__d_z_h = 1\n",
    "        \n",
    "        # [n_examples, n_features]\n",
    "        d_z_h__d_w_h = x\n",
    "        \n",
    "        # output dim: [n_hidden, n_features]\n",
    "        d_loss__d_w_h = np.dot((d_loss__a_h * d_a_h__d_z_h).T, d_z_h__d_w_h)\n",
    "        d_loss__d_b_h = np.sum((d_loss__a_h * d_a_h__d_z_h), axis=0)\n",
    "\n",
    "        return (d_loss__dw_out, d_loss__db_out, d_loss__d_w_h, d_loss__d_b_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea4db63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(onehot_targets, probas):\n",
    "    #print(f\"onehot_targets = {onehot_targets}\")\n",
    "    #print(f\"probas = {probas}\")\n",
    "    mse = np.mean((onehot_targets - probas)**2)\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b925d77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11858    0\n",
       "4818     0\n",
       "19478    1\n",
       "5539     0\n",
       "20738    0\n",
       "        ..\n",
       "3746     0\n",
       "14618    0\n",
       "17947    0\n",
       "11690    0\n",
       "21290    1\n",
       "Name: income, Length: 5210, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5ed917f",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "y_hot_train = encoder.fit_transform(y_train.values.reshape(-1, 1))\n",
    "y_hot_test = encoder.transform(y_test.values.reshape(-1, 1))\n",
    "x = np.array([[1, 2]])\n",
    "bias_h = np.array([[0.1, 0.1]])\n",
    "bias_out = np.array([[0.2, 0.3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cfa85d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, X_train, X_test, y_onehot_train, y_onehot_test, num_epochs, num_classes, learning_rate=0.01):\n",
    "\n",
    "    \"\"\"\n",
    "    Training Simple MLP modle using entire data set (no train/test split)\n",
    "    \"\"\"\n",
    "  \n",
    "    epoch_loss_train = []\n",
    "    epoch_loss_test = []\n",
    "    \n",
    "    for e in range(num_epochs):\n",
    "        print(f\"****Epoch: {e}****\")\n",
    "\n",
    "        #### Compute outputs ####\n",
    "        a_h, a_out = model.forward(X_train)\n",
    "\n",
    "        #### Compute gradients ####\n",
    "        d_loss__d_w_out, d_loss__d_b_out, d_loss__d_w_h, d_loss__d_b_h = model.backward(X_train, a_h, a_out, y_onehot_train)\n",
    "\n",
    "        #### Update weights ####\n",
    "        model.weight_h -= learning_rate * d_loss__d_w_h\n",
    "        model.bias_h -= learning_rate * d_loss__d_b_h\n",
    "        model.weight_out -= learning_rate * d_loss__d_w_out\n",
    "        model.bias_out -= learning_rate * d_loss__d_b_out\n",
    "\n",
    "        ### Look at test dataset ###\n",
    "        _, a_out_test = model.forward(X_test)\n",
    "    \n",
    "        #### Epoch Logging ####        \n",
    "        mseTrain = mse_loss(y_onehot_train, a_out)\n",
    "        mseTest = mse_loss(y_onehot_test, a_out_test)\n",
    "        epoch_loss_train.append(mseTrain)\n",
    "        epoch_loss_test.append(mseTest)\n",
    "        print(f'MSE Train: {mseTrain:.1f}')\n",
    "        print(f'MSE Test: {mseTest:.1f}')\n",
    "\n",
    "    return epoch_loss_train, epoch_loss_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6cc8198e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating random weights for weight_h\n",
      "creating random weights for bias_h\n",
      "creating random weights for weight_out\n",
      "creating random weights for bias_out\n"
     ]
    }
   ],
   "source": [
    "model = SimpleMLP(num_features=len(X_train.columns), num_hidden=2, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "24263ad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****Epoch: 0****\n",
      "MSE Train: 0.4\n",
      "MSE Test: 10237658.5\n",
      "****Epoch: 1****\n",
      "MSE Train: 10307049.6\n",
      "MSE Test: 100110489674944197927835611879702528.0\n",
      "****Epoch: 2****\n",
      "MSE Train: 99505957628026404988086907855437824.0\n",
      "MSE Test: 94217598234864560123428139251271320708185420661012760012049102121676318939118083897014709384849676620960792948168982528.0\n"
     ]
    }
   ],
   "source": [
    "epoch_loss_train, epoch_loss_test = train(model, X_train, X_test, y_hot_train, y_hot_test, num_epochs=3, learning_rate=0.01, num_classes = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a77932c",
   "metadata": {},
   "source": [
    "## Reflection and Conceptual Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98960112",
   "metadata": {},
   "source": [
    "### Why did you choose the specific architecture (e.g., number of layers, activation functions) for each model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a3589c",
   "metadata": {},
   "source": [
    "### How did you monitor and mitigate overfitting in your models?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0b0692",
   "metadata": {},
   "source": [
    "### What ethical concerns might arise from deploying models trained on these datasets?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31ee327",
   "metadata": {},
   "source": [
    "### Why are activation functions necessary in neural networks?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
